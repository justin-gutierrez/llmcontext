#!/usr/bin/env python3
"""
Test script for Ollama summarization of documentation chunks.
"""

import sys
import os

# Add the project root to the Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from llmcontext.summarizer.ollama import summarize_with_ollama


def load_markdown_chunk(file_path: str, max_chars: int = 1000) -> str:
    """
    Load a markdown file and return the first N characters.
    
    Args:
        file_path: Path to the markdown file
        max_chars: Maximum number of characters to return
        
    Returns:
        The first N characters of the markdown content
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            return content[:max_chars]
    except FileNotFoundError:
        print(f"ERROR: File not found: {file_path}")
        return ""
    except Exception as e:
        print(f"ERROR: Failed to read file {file_path}: {e}")
        return ""


def format_summarization_prompt(chunk: str, tool_name: str = "Tailwind") -> str:
    """
    Format the markdown chunk into a summarization prompt.
    
    Args:
        chunk: The markdown content chunk
        tool_name: Name of the tool/framework
        
    Returns:
        Formatted prompt for summarization
    """
    prompt = f"""Here is a chunk of {tool_name} documentation. Please summarize it for LLM consumption:
- Remove fluff and marketing
- Focus on config, usage, errors, and examples
- Output clean, concise Markdown
---
{chunk}"""
    return prompt


def save_result_to_file(original_chunk: str, summary: str, tool_name: str):
    """
    Save the original chunk and summary to a markdown file for manual review.
    
    Args:
        original_chunk: The original documentation chunk
        summary: The summarized content
        tool_name: Name of the tool/framework
    """
    import os
    
    # Create the directory structure if it doesn't exist
    output_dir = f"docs/{tool_name}"
    os.makedirs(output_dir, exist_ok=True)
    
    # Create the output file path
    output_file = f"{output_dir}/test-summary.md"
    
    # Create the content with clear sections
    content = f"""# {tool_name.title()} Documentation Summarization Test

## Test Information
- **Date**: {os.popen('date').read().strip() if os.name != 'nt' else 'Windows'}
- **Tool**: {tool_name}
- **Model**: mistral
- **Chunk Size**: {len(original_chunk)} characters
- **Summary Size**: {len(summary)} characters
- **Compression Ratio**: {len(summary) / len(original_chunk) * 100:.1f}%

## Original Chunk

```markdown
{original_chunk}
```

## Generated Summary

```markdown
{summary}
```

## Analysis

### Original Content Overview
- **Lines**: {original_chunk.count(chr(10)) + 1}
- **Words**: {len(original_chunk.split())}
- **Characters**: {len(original_chunk)}

### Summary Content Overview
- **Lines**: {summary.count(chr(10)) + 1}
- **Words**: {len(summary.split())}
- **Characters**: {len(summary)}

### Key Observations
- Compression achieved: {len(original_chunk) - len(summary)} characters removed
- Information retention: {len(summary) / len(original_chunk) * 100:.1f}% of original size
- Summary focuses on: configuration, usage examples, and practical implementation

---
*Generated by LLMContext Ollama Summarization Test*
"""
    
    # Write the content to file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)


def test_ollama_summarization():
    """Test Ollama summarization with a Tailwind documentation chunk."""
    print("=== Ollama Documentation Summarization Test ===\n")
    
    # Load the markdown chunk
    file_path = "raw_docs/tailwind.md"
    print(f"1. Loading markdown chunk from: {file_path}")
    
    chunk = load_markdown_chunk(file_path, max_chars=1000)
    if not chunk:
        print("ERROR: Failed to load markdown chunk")
        return False
    
    print(f"   SUCCESS: Loaded {len(chunk)} characters")
    
    # Print the original chunk
    print("\n" + "="*50)
    print("ORIGINAL CHUNK:")
    print("="*50)
    print(chunk)
    print("="*50)
    
    # Format the prompt
    print("\n2. Formatting summarization prompt...")
    prompt = format_summarization_prompt(chunk, "Tailwind")
    print(f"   SUCCESS: Created prompt ({len(prompt)} characters)")
    
    # Call Ollama summarization
    print("\n3. Calling Ollama summarization...")
    try:
        summary = summarize_with_ollama(prompt)
        print(f"   SUCCESS: Got summary ({len(summary)} characters)")
        
        # Print the summary
        print("\n" + "="*50)
        print("SUMMARY:")
        print("="*50)
        print(summary)
        print("="*50)
        
        # Save the result to file
        print("\n4. Saving result to file...")
        save_result_to_file(chunk, summary, "tailwind")
        print("   SUCCESS: Result saved to docs/tailwind/test-summary.md")
        
        return True
        
    except Exception as e:
        print(f"   ERROR: Summarization failed: {e}")
        return False


def test_with_different_models():
    """Test summarization with different Ollama models."""
    print("\n=== Testing Different Models ===\n")
    
    # Load a smaller chunk for faster testing
    chunk = load_markdown_chunk("raw_docs/tailwind.md", max_chars=500)
    if not chunk:
        return False
    
    prompt = format_summarization_prompt(chunk, "Tailwind")
    
    # Test with different models
    models_to_test = ["mistral", "llama2", "codellama"]
    
    for model in models_to_test:
        print(f"Testing with model: {model}")
        try:
            summary = summarize_with_ollama(prompt, model=model)
            print(f"   SUCCESS: {model} summary ({len(summary)} characters)")
            print(f"   Preview: {summary[:100]}...")
            
            # Save result for this model
            save_result_to_file(chunk, summary, f"tailwind-{model}")
            print(f"   SAVED: Result saved to docs/tailwind-{model}/test-summary.md")
            
        except Exception as e:
            print(f"   ERROR: {model} failed - {e}")
        print()


if __name__ == "__main__":
    # Test basic summarization
    if test_ollama_summarization():
        print("\n‚úÖ Basic summarization test PASSED")
        
        # Ask if user wants to test different models
        try:
            response = input("\nTest with different models? (y/n): ").lower().strip()
            if response in ['y', 'yes']:
                test_with_different_models()
        except KeyboardInterrupt:
            print("\n\nTest completed.")
        
        print("\nüéâ Ollama summarization test completed successfully!")
    else:
        print("\n‚ùå Basic summarization test FAILED")
        print("\nMake sure:")
        print("1. Ollama is running on localhost:11434")
        print("2. The 'mistral' model is available")
        print("3. The file 'raw_docs/tailwind.md' exists")
        sys.exit(1) 